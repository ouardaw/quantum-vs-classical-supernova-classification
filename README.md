
Systematic Comparison of Classical vs Near-Term Quantum Machine Learning for Astronomical Classification

ğŸŒŒ Why This Project Exists

From Childhood Wonder to Strategic Quantum Exploration

My fascination with the cosmos began earlyâ€”sparked by Hubert Reevesâ€™ PoussiÃ¨re dâ€™Ã‰toiles and Carl Saganâ€™s Cosmos. Over the years, I pursued this curiosity relentlessly: reading about quantum field theory, many-worlds interpretation, black holes, dark matter, and M-theory; watching countless space documentaries; and learning from science communicators like Neil deGrasse Tyson, Janna Levin, Sean Carroll, and Brian Cox.

A New Obsession Emerges

In recent years, I became equally fascinated by quantum computingâ€”not as a buzzword, but as a fundamentally new computational paradigm with real (and very specific) potential. After more than 10 years in product management, I wanted to deepen my hands-on technical understanding of emerging technologies, not just their strategic implications.

I completed IBMâ€™s Basics of Quantum Information certification, strengthened my machine-learning foundations, and decided to combine these skills with my lifelong interest in astrophysics.

The Real-World Context

Modern astronomical surveys operate at extreme scale:
	â€¢	Zwicky Transient Facility (ZTF): ~10,000 alerts/night
	â€¢	ALeRCE & ANTARES: production ML systems using Random Forests, gradient boosting, and deep learning
	â€¢	Vera Rubin Observatory (LSST): expected to generate ~10 million alerts/night
	â€¢	PLAsTiCC Challenge: created by LSST scientists to benchmark ML approaches at this scale

Classical ML already works extremely well here. This project does not attempt to outperform professional astronomy pipelines.

â¸»

â“ The Research Question

Instead, I asked a more fundamental question:

Given that classical ML works well for astronomical transient classification, what would it take for quantum ML to be competitive?

This is a technology-fit evaluation, not an astronomy optimization task. I used real PLAsTiCC data as a realistic testbed to understand where near-term quantum ML helpsâ€”and where it does not.

â¸»

ğŸ§ª Methodology Overview (What We Implemented Today)

I implemented parallel, consistent pipelines with leakage-safe evaluation:

Classical ML (Baselines)
	â€¢	Logistic Regression (scaled)
	â€¢	Random Forest (raw)
	â€¢	CatBoost (raw)
	â€¢	Soft-voting ensemble (LR + RF + CatBoost)

Quantum ML (Near-term QML)
	â€¢	3-qubit variational quantum classifier (VQC)
	â€¢	Qiskit EstimatorQNN
	â€¢	COBYLA optimizer (gradient-free)
	â€¢	StatevectorEstimator (V2 primitive) for noiseless simulation (no deprecation warnings)

âœ… Leakage-Safe Design (Key Fix)

To avoid unintentionally â€œpeekingâ€ at the test set, the final pipelines follow this order:

split â†’ select features (train only) â†’ fit preprocessing (train only) â†’ apply to test â†’ train â†’ evaluate

This applies to:
	â€¢	correlation-based feature selection
	â€¢	outlier clipping (p1/p99)
	â€¢	conditional log transform
	â€¢	scaling (standardization or angle mapping)
	â€¢	threshold selection (done on train only for the quantum model)

â¸»

ğŸ“Š Results Summary (Updated)

Dataset: 1,072 PLAsTiCC transients
	â€¢	523 Type Ia (SNIa)
	â€¢	549 Type II (SNII)

Main Comparison (Realistic QML Constraint: 3 features / 3 qubits)

Approach	Model	Features	Accuracy	AUC	Notes
Classical	Ensemble (LR+RF+CatBoost)	16	0.744	0.853	Best overall baseline
Classical	Ensemble (LR+RF+CatBoost)	3	0.609	0.638	Apples-to-apples vs quantum
Quantum	3-qubit EstimatorQNN	3	0.591	0.602	Threshold chosen on TRAIN

Classical (16 features) breakdown
	â€¢	Logistic Regression: Accuracy 0.712, AUC 0.770
	â€¢	Random Forest: Accuracy 0.758, AUC 0.845
	â€¢	CatBoost: Accuracy 0.744, AUC 0.846
	â€¢	Ensemble: Accuracy 0.744, AUC 0.853

Quantum (3 qubits / 3 features) diagnostic metrics
	â€¢	Accuracy: 0.591
	â€¢	AUC: 0.602
	â€¢	Balanced accuracy: 0.594
	â€¢	Sensitivity (SNIa recall): 0.714
	â€¢	Specificity (SNII recall): 0.473
	â€¢	Decision threshold: 0.20 (selected from TRAIN to maximize balanced accuracy)

Confusion matrix (SNII, SNIa):

[[52 58]
 [30 75]]


â¸»

ğŸ” Root Cause Insight (Todayâ€™s Main Takeaway)

The most important result wasnâ€™t â€œquantum lostâ€ or â€œquantum won.â€

It was this:

When both classical and quantum models are restricted to the same 3 features, their performance becomes very close.
The biggest drop happens when you compress 16 features â†’ 3 features, not when you switch classical â†’ quantum.

	â€¢	Classical ensemble: 0.744 â†’ 0.609 accuracy when forced to use only 3 features
	â€¢	Quantum QNN on the same 3 features: 0.591 accuracy

Conclusion: the dominant bottleneck is information loss from feature compression (a practical NISQ constraint), not necessarily â€œquantum vs classicalâ€ in isolation.

â¸»

ğŸ§  Feature Selection Used for Quantum (Train-Only)

Because 3 qubits â‰ˆ 3 input dimensions, features were selected automatically using point-biserial correlation computed on the training split only:

Top 3 features (train-only):
	â€¢	time_span (|corr| â‰ˆ 0.279)
	â€¢	decline_time (|corr| â‰ˆ 0.266)
	â€¢	mag_max (|corr| â‰ˆ 0.161)

The same selected features were used for:
	â€¢	the quantum classifier
	â€¢	the classical Top-3 baseline (apples-to-apples)

â¸»

âš›ï¸ What This Suggests About Near-Term QML

Quantum ML is not universally superior to classical ML.
Knowing when not to use it is just as important as knowing how to implement it.

Quantum ML is most promising when:
	â€¢	the problem naturally has a small number of high-signal features
	â€¢	feature separation is strong (often > 0.5Ïƒ per feature)
	â€¢	you can scale to more qubits / richer embeddings without collapsing trainability
	â€¢	thereâ€™s structure that might benefit from quantum representations (kernels / embeddings / physics structure)

Classical ML remains optimal when:
	â€¢	performance comes from combining many weak signals across higher dimensions
	â€¢	ensembles and boosting can exploit feature interactions easily
	â€¢	you want mature, interpretable, production-ready workflows

â¸»

ğŸ“‚ Project Structure

quantum-vs-classical-supernova-classification/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ plasticc/ User-created directory â”‚ # Place raw PLAsTiCC data her
â”‚       
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 00_explore_plasticc.ipynb
â”‚   â”œâ”€â”€ 01_feature_extraction.ipynb
â”‚   â”œâ”€â”€ 02_classical_ml.ipynb
â”‚   â””â”€â”€ 03_quantum_classifier.ipynb
â”œâ”€â”€ results/
â”‚   
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ LICENSE


â¸»

ğŸš€ Setup & Data Access

PLAsTiCC data is not included.

To run the notebooks:
	1.	Create a Kaggle account
	2.	Accept the PLAsTiCC-2018 competition terms
	3.	Download the dataset from Kaggle
	4.	Create data/plasticc/ and place the files there
	5.	Run the feature extraction notebook to generate transient_features.csv

This repository complies with Kaggleâ€™s data-usage requirements.

â¸»

ğŸ‘¤ Author

Ouarda Wilson
Senior Product Manager with hands-on experience in applied ML and quantum computing
	â€¢	10+ years delivering complex technical products
	â€¢	Background in AI/ML systems and data-driven decision making
	â€¢	Practical quantum computing experience with Qiskit
	â€¢	Interests: quantum-classical hybrids, real-world applicability of emerging tech, scientific computing

ğŸ”— GitHub: https://github.com/ouardaw/quantum-vs-classical-supernova-classification
ğŸ”— LinkedIn: https://www.linkedin.com/in/ouarda-jw/

â¸»

â­ Acknowledgments
	â€¢	IBM Quantum â€” Qiskit framework and educational resources
	â€¢	Anthropic & OpenAI â€” AI assistance for reasoning support, code review, and documentation
	â€¢	PLAsTiCC Organizers â€” dataset creation
	â€¢	Kaggle â€” hosting and infrastructure
	â€¢	Kaggle Community â€” baselines and shared insights

â¸»

Built with quantum curiosity, classical rigor, and thoughtful AI assistance âš›ï¸ğŸ”­

â¸»

