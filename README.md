

Systematic Comparison of Classical vs Near-Term Quantum Machine Learning for Astronomical Classification

â¸»

ğŸŒŒ Why This Project Exists

From Childhood Wonder to Strategic Quantum Exploration

My fascination with the cosmos began earlyâ€”sparked by Hubert Reevesâ€™ PoussiÃ¨re dâ€™Ã‰toiles and Carl Saganâ€™s Cosmos. Over the years, I pursued this curiosity relentlessly: reading about quantum field theory, many-worlds interpretation, black holes, dark matter, and M-theory; watching countless space documentaries; and learning from science communicators like Neil deGrasse Tyson, Janna Levin, Sean Carroll, and Brian Cox.

A New Obsession Emerges

In recent years, I became equally fascinated by quantum computingâ€”not as a buzzword, but as a fundamentally new computational paradigm with real (and very specific) potential. After more than 10 years in product management, I wanted to deepen my hands-on technical understanding of emerging technologies, not just their strategic implications.

I completed IBMâ€™s Basics of Quantum Information certification, strengthened my machine-learning foundations, and decided to combine these skills with my lifelong interest in astrophysics.

The Real-World Context

Modern astronomical surveys operate at extreme scale:
	â€¢	Zwicky Transient Facility (ZTF): ~10,000 alerts per night
	â€¢	ALeRCE & ANTARES: Production ML systems using Random Forests, gradient boosting, and deep learning
	â€¢	Vera Rubin Observatory (LSST): Expected to generate ~10 million alerts per night
	â€¢	PLAsTiCC Challenge: Created by LSST scientists to benchmark ML approaches at this scale

Classical ML already works extremely well here.
This project does not attempt to outperform professional astronomy pipelines.

â¸»

â“ The Research Question

Instead, I asked a more fundamental question:

Given that classical ML works well for astronomical transient classification, what would it take for quantum ML to be competitive?

This is a technology-fit evaluation, not an astronomy optimization task.
I used real PLAsTiCC data as a realistic testbed to understand where near-term quantum ML helps â€” and where it does not.

â¸»

ğŸ§ª Methodology Overview

I implemented parallel, production-quality pipelines:
	â€¢	Classical ML
	â€¢	Logistic Regression
	â€¢	Random Forest
	â€¢	CatBoost
	â€¢	Soft-voting ensemble
	â€¢	Quantum ML
	â€¢	3-qubit variational quantum classifier
	â€¢	Qiskit EstimatorQNN
	â€¢	COBYLA optimizer

Both approaches were trained and evaluated on the same dataset, using consistent splits and metrics.

â¸»

ğŸ“Š Results Summary (Updated)

Dataset: 1,072 PLAsTiCC transients
	â€¢	523 Type Ia (SNIa)
	â€¢	549 Type II (SNII)

Approach	Model	Features	Accuracy	AUC	Training Time
Classical	Ensemble (LR + RF + CatBoost)	16	74.4%	0.852	~2 min
Quantum	3-qubit EstimatorQNN	3	50.2%	0.576	~10â€“11 min
Baseline	Random guessing	â€“	50.0%	0.500	â€“

Classical Performance
	â€¢	Random Forest: 75.8% accuracy
	â€¢	CatBoost: 74.4% accuracy
	â€¢	Logistic Regression: 71.2% accuracy
	â€¢	Ensemble: 74.4% accuracy, best AUC

Quantum Performance
	â€¢	Accuracy: 50.2%
	â€¢	AUC: 0.576
	â€¢	Sensitivity (SNIa recall): 87.6%
	â€¢	Specificity (SNII recall): 14.5%
	â€¢	Balanced accuracy: 51.1%

Interpretation:
The quantum model learned a strong bias toward predicting SNIa rather than a balanced discriminative boundary â€” a clear symptom of weak feature separability under tight qubit constraints.

â¸»

ğŸ”¬ Experimental Iteration & Model Diagnostics

<details>
<summary><b>Systematic experimentation, diagnostics, and limits analysis (click to expand)</b></summary>


This project evolved through controlled, hypothesis-driven iterations to understand how near-term quantum ML behaves on real scientific data.

Initial Baseline (600 samples)
	â€¢	Classical: ~75.0% accuracy
	â€¢	Quantum: ~47.5% accuracy
	â€¢	Gap: âˆ’27.5 pp

Improvements Applied
	â€¢	Dataset scaling: 600 â†’ 1,072 samples
	â€¢	Auto-selection of top 3 features via correlation analysis
	â€¢	Outlier-robust preprocessing (clipping + log transforms)
	â€¢	Quantum-specific scaling to [0, \pi]
	â€¢	Deeper circuit and more training iterations

Final Outcome (1,072 samples)
	â€¢	Quantum accuracy improved slightly to ~50.2%
	â€¢	Classical performance remained stable
	â€¢	Performance plateaued despite tuning

Why Performance Plateaued

Feature analysis revealed a fundamental data limitation:

Feature	Separation (Ïƒ)
Best features	0.31â€“0.46Ïƒ
Typical requirement for quantum ML	>0.5Ïƒ

No amount of circuit depth or optimizer tuning can compensate for insufficient feature separability.

Key takeaway:

Quantum ML performance is primarily constrained by feature quality, not model complexity or dataset size alone.

</details>



â¸»

ğŸ” Root Cause Analysis: Feature Quality

Feature	Correlation	Separation (Ïƒ)
time_span	0.280	0.46
decline_time	0.269	0.44
mag_max	0.151	0.31
rise_decline_ratio	0.017	0.03

Effective quantum learning typically requires >0.5Ïƒ separation per encoded feature.
This dataset provides 0.03â€“0.46Ïƒ, explaining the observed performance ceiling.

â¸»

âš›ï¸ Why Classical Outperformed Quantum Here

Classical ensemble methods succeed because they:
	â€¢	Combine many weak signals across 16 dimensions
	â€¢	Learn flexible, non-linear decision boundaries
	â€¢	Compensate for poor individual features via ensemble voting

Quantum ML struggled because it:
	â€¢	Is constrained to 3 features (3 qubits)
	â€¢	Requires strong individual feature signal
	â€¢	Suffers from weak gradients when features overlap

This is not a failure of quantum computing â€” it is a problemâ€“tool mismatch.

â¸»

ğŸ’¡ Strategic Insight

Quantum ML is not universally superior to classical ML.
Knowing when not to use it is just as important as knowing how to implement it.

Quantum ML is most promising when:
	â€¢	Feature separation is strong (>1Ïƒ)
	â€¢	Datasets are large (10k+ samples)
	â€¢	Problem structure is quantum-native

Classical ML remains optimal when:
	â€¢	Datasets are small to mid-sized
	â€¢	Features are weakly separable
	â€¢	The domain is mature and well-understood

â¸»

ğŸ“‚ Project Structure

quantum-transient-detector_Real_Data/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ plasticc/                 # User-created directory
â”‚                                # Place raw PLAsTiCC data here
â”‚                                # Engineered features are also saved here
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 00_explore_plasticc.ipynb
â”‚   â”œâ”€â”€ 01_feature_extraction.ipynb
â”‚   â”œâ”€â”€ 02_classical_ml.ipynb
â”‚   â””â”€â”€ 03_quantum_classifier.ipynb
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ plasticc_classical_results.json
â”‚   â””â”€â”€ plasticc_quantum_results_final.json
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ LICENSE


â¸»

ğŸš€ Setup & Data Access

PLAsTiCC data is not included.

To run the notebooks:
	1.	Create a Kaggle account
	2.	Accept the PLAsTiCC-2018 competition terms
	3.	Download the dataset from Kaggle
	4.	Create data/plasticc/ and place the files there

This repository fully complies with Kaggleâ€™s data-usage requirements.

â¸»

ğŸ‘¤ Author

Ouarda Wilson
Senior Product Manager with hands-on experience in applied ML and quantum computing
	â€¢	10+ years delivering complex technical products
	â€¢	Background in AI/ML systems and data-driven decision making
	â€¢	Practical quantum computing experience with Qiskit
	â€¢	Interests: quantum-classical hybrids, real-world applicability of emerging tech, scientific computing

ğŸ”— GitHub: https://github.com/ouardaw/quantum-vs-classical-supernova-classification
ğŸ”— LinkedIn: https://www.linkedin.com/in/ouarda-jw/

â¸»

â­ Acknowledgments
	â€¢	IBM Quantum â€” Qiskit framework and educational resources
	â€¢	Anthropic & OpenAI â€” AI assistance for reasoning support, code review, and documentation
	â€¢	PLAsTiCC Organizers â€” Dataset creation
	â€¢	Kaggle â€” Hosting and infrastructure
	â€¢	Kaggle Community â€” Baselines and shared insights

â¸»

Built with quantum curiosity, classical rigor, and thoughtful AI assistance âš›ï¸ğŸ”­

â¸»
