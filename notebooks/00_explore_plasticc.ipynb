{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30487212-1067-456e-ae96-5a29af7f7c03",
   "metadata": {},
   "source": [
    "## What this notebook does (Dataset Sampling & Preparation)\n",
    "\n",
    "This notebook creates a **manageable, balanced subset** of the full PLAsTiCC dataset that can be processed locally and used consistently across all notebooks.\n",
    "\n",
    "### 1) Configuration: dataset size\n",
    "- Sets `N_PER_CLASS = 1000`, meaning:\n",
    "  - 1,000 Type Ia supernovae (SNIa, target = 90)\n",
    "  - 1,000 Type II supernovae (SNII, target = 42)\n",
    "- This increases statistical stability compared to earlier smaller samples while remaining computationally feasible.\n",
    "\n",
    "### 2) Load full PLAsTiCC metadata\n",
    "- Loads `training_set_metadata.csv`, which contains one row per astronomical object.\n",
    "- Filters the metadata to only the two classes of interest (SNIa vs SNII).\n",
    "- Prints how many total examples of each class are available in the full dataset.\n",
    "\n",
    "### 3) Balanced sampling by class\n",
    "- Randomly samples up to `N_PER_CLASS` objects from each class using a fixed random seed for reproducibility.\n",
    "- Combines both classes into a single balanced metadata table (`sample_metadata`).\n",
    "- Ensures:\n",
    "  - No class imbalance\n",
    "  - Identical object set is reused across feature extraction, classical ML, and quantum ML\n",
    "\n",
    "### 4) Save sampled metadata\n",
    "- Writes `sample_metadata.csv`, which lists the exact objects included in the study.\n",
    "- This file defines the **ground truth object set** for all downstream notebooks.\n",
    "\n",
    "### 5) Extract corresponding light curves\n",
    "- Reads the very large `training_set.csv` file **in chunks** to avoid loading it all into memory.\n",
    "- For each chunk:\n",
    "  - Keeps only rows belonging to the sampled object IDs\n",
    "- Concatenates all matching rows into `sample_lightcurves.csv`.\n",
    "\n",
    "### 6) Save sampled light curves\n",
    "- Writes `sample_lightcurves.csv`, containing all time-series observations for the sampled objects.\n",
    "- This dramatically reduces data size while preserving realistic, irregular astronomical light curves.\n",
    "\n",
    "### Why this step matters\n",
    "- Keeps the project **reproducible**: everyone uses the same object subset\n",
    "- Keeps it **practical**: avoids multi-GB processing in later notebooks\n",
    "- Preserves **scientific validity**: real, unmodified PLAsTiCC observations\n",
    "- Enables fair comparison between classical and quantum models on identical data\n",
    "\n",
    "**Next step:** run the feature extraction notebook to convert these light curves into engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4969ffc1-3d77-45cc-b7e2-12c814816e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1000 samples per class = 2000 total\n",
      "Total objects in metadata: 7848\n",
      "\n",
      "Available samples:\n",
      "  SNIa (90): 2313\n",
      "  SNII (42): 1193\n",
      "\n",
      "âœ“ Sampled dataset:\n",
      "  Total: 2000\n",
      "  SNIa: 1000\n",
      "  SNII: 1000\n",
      "\n",
      "âœ“ Saved: ../data/plasticc/sample_metadata.csv\n",
      "\n",
      "ðŸ“Š Extracting lightcurves for 2000 objects...\n",
      "  Processed 0 rows...\n",
      "  Processed 1,000,000 rows...\n",
      "\n",
      "âœ“ Total lightcurve rows: 381,810\n",
      "âœ“ Saved: ../data/plasticc/sample_lightcurves.csv\n",
      "\n",
      "======================================================================\n",
      "DATASET GENERATION COMPLETE!\n",
      "======================================================================\n",
      "Next step: Run feature extraction notebook (02_)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - INCREASED DATASET SIZE\n",
    "# ============================================================================\n",
    "\n",
    "N_PER_CLASS = 1000  \n",
    "\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"plasticc\")\n",
    "META_PATH = os.path.join(DATA_DIR, \"training_set_metadata.csv\")\n",
    "LC_PATH = os.path.join(DATA_DIR, \"training_set.csv\")\n",
    "\n",
    "print(f\"Target: {N_PER_CLASS} samples per class = {N_PER_CLASS * 2} total\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD FULL METADATA\n",
    "# ============================================================================\n",
    "\n",
    "metadata = pd.read_csv(META_PATH)\n",
    "print(f\"Total objects in metadata: {len(metadata)}\")\n",
    "\n",
    "# Focus on SNIa (90) vs SNII (42)\n",
    "SELECTED_CLASSES = [90, 42]\n",
    "binary_metadata = metadata[metadata[\"target\"].isin(SELECTED_CLASSES)].copy()\n",
    "\n",
    "print(f\"\\nAvailable samples:\")\n",
    "print(f\"  SNIa (90): {(binary_metadata['target'] == 90).sum()}\")\n",
    "print(f\"  SNII (42): {(binary_metadata['target'] == 42).sum()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAMPLE 1000 OF EACH CLASS\n",
    "# ============================================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "snia_all = binary_metadata[binary_metadata[\"target\"] == 90]\n",
    "snii_all = binary_metadata[binary_metadata[\"target\"] == 42]\n",
    "\n",
    "# Take 1000 of each (or max available)\n",
    "n_snia = min(N_PER_CLASS, len(snia_all))\n",
    "n_snii = min(N_PER_CLASS, len(snii_all))\n",
    "\n",
    "snia_sample = snia_all.sample(n_snia, random_state=42)\n",
    "snii_sample = snii_all.sample(n_snii, random_state=42)\n",
    "\n",
    "sample_metadata = pd.concat([snia_sample, snii_sample], ignore_index=True)\n",
    "\n",
    "print(f\"\\nâœ“ Sampled dataset:\")\n",
    "print(f\"  Total: {len(sample_metadata)}\")\n",
    "print(f\"  SNIa: {(sample_metadata['target'] == 90).sum()}\")\n",
    "print(f\"  SNII: {(sample_metadata['target'] == 42).sum()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE SAMPLE METADATA\n",
    "# ============================================================================\n",
    "\n",
    "SAMPLE_META_PATH = os.path.join(DATA_DIR, \"sample_metadata.csv\")\n",
    "sample_metadata.to_csv(SAMPLE_META_PATH, index=False)\n",
    "print(f\"\\nâœ“ Saved: {SAMPLE_META_PATH}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACT LIGHTCURVES FOR SAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "sample_ids = sample_metadata[\"object_id\"].values\n",
    "sample_ids_set = set(sample_ids)\n",
    "\n",
    "print(f\"\\nðŸ“Š Extracting lightcurves for {len(sample_ids_set)} objects...\")\n",
    "\n",
    "chunksize = 100000\n",
    "collected_chunks = []\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(LC_PATH, chunksize=chunksize)):\n",
    "    mask = chunk[\"object_id\"].isin(sample_ids_set)\n",
    "    sub = chunk[mask]\n",
    "    if not sub.empty:\n",
    "        collected_chunks.append(sub)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"  Processed {i * chunksize:,} rows...\")\n",
    "\n",
    "if collected_chunks:\n",
    "    sample_lightcurves = pd.concat(collected_chunks, ignore_index=True)\n",
    "else:\n",
    "    sample_lightcurves = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nâœ“ Total lightcurve rows: {len(sample_lightcurves):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE SAMPLE LIGHTCURVES\n",
    "# ============================================================================\n",
    "\n",
    "SAMPLE_LC_PATH = os.path.join(DATA_DIR, \"sample_lightcurves.csv\")\n",
    "sample_lightcurves.to_csv(SAMPLE_LC_PATH, index=False)\n",
    "print(f\"âœ“ Saved: {SAMPLE_LC_PATH}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATASET GENERATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Next step: Run feature extraction notebook (02_)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033a42a-4a8f-46e5-a3cd-f63ac15c42b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
