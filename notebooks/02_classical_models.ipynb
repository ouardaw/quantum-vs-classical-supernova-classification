{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2c17e3-1d4a-4296-b715-3f1f6bed7759",
   "metadata": {},
   "source": [
    "What this notebook does (Classical Baselines)\n",
    "\n",
    "This notebook builds classical ML baselines for PLAsTiCC (SNIa vs SNII) to benchmark performance against the quantum classifier.\n",
    "\t1.\tLoad engineered features\n",
    "\tâ€¢\tReads ../data/plasticc/transient_features.csv (one row per transient + 16 engineered features + label).\n",
    "\t2.\tPrepare labels + feature matrix\n",
    "\tâ€¢\tUses 16 engineered features.\n",
    "\tâ€¢\tEncodes labels: SNII â†’ 0, SNIa â†’ 1.\n",
    "\t3.\tTrain/test split (leakage-safe)\n",
    "\tâ€¢\tSplits once into 80% train / 20% test with stratification.\n",
    "\tâ€¢\tAll preprocessing decisions that require â€œfittingâ€ are done on train only (via pipelines or explicit train-fit steps).\n",
    "\t4.\tTrain classical baselines (16 features)\n",
    "\tâ€¢\tLogistic Regression (StandardScaler inside pipeline)\n",
    "\tâ€¢\tRandom Forest (no scaling required)\n",
    "\tâ€¢\tCatBoost (optional, if installed)\n",
    "\tâ€¢\tSoft Voting Ensemble (combines the above using predicted probabilities)\n",
    "\t5.\tEvaluate + save results\n",
    "\tâ€¢\tReports Accuracy, Balanced Accuracy, ROC AUC, and confusion matrices.\n",
    "\tâ€¢\tSaves metrics to ../results/plasticc_classical_results_2k.json.\n",
    "\t6.\tTop-3 â€œapples-to-applesâ€ baseline (optional section)\n",
    "\tâ€¢\tSelects the top 3 features (same criterion as quantum) using TRAIN only.\n",
    "\tâ€¢\tRuns the same classical models using only those 3 features.\n",
    "\tâ€¢\tSaves results to ../results/plasticc_classical_top3_results.json.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac179cc-0ccf-4471-8313-8fdc34a4ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1072 samples with 18 columns\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "SNII    549\n",
      "SNIa    523\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Feature matrix: (1072, 16)\n",
      "Labels: (1072,), distribution: (array([0, 1]), array([549, 523]))\n",
      "\n",
      "Train: 857, Test: 215\n",
      "\n",
      "======================================================================\n",
      "TRAINING + EVALUATION (CLASSICAL)\n",
      "======================================================================\n",
      "\n",
      "Logistic Regression\n",
      "--------------------------------------------------\n",
      "Accuracy:          0.712\n",
      "Balanced accuracy: 0.712\n",
      "AUC:               0.770\n",
      "Confusion matrix [SNII, SNIa]:\n",
      "[[75 35]\n",
      " [27 78]]\n",
      "\n",
      "Random Forest\n",
      "--------------------------------------------------\n",
      "Accuracy:          0.758\n",
      "Balanced accuracy: 0.759\n",
      "AUC:               0.845\n",
      "Confusion matrix [SNII, SNIa]:\n",
      "[[81 29]\n",
      " [23 82]]\n",
      "\n",
      "CatBoost\n",
      "--------------------------------------------------\n",
      "Accuracy:          0.744\n",
      "Balanced accuracy: 0.744\n",
      "AUC:               0.846\n",
      "Confusion matrix [SNII, SNIa]:\n",
      "[[83 27]\n",
      " [28 77]]\n",
      "\n",
      "Ensemble\n",
      "--------------------------------------------------\n",
      "Accuracy:          0.744\n",
      "Balanced accuracy: 0.745\n",
      "AUC:               0.853\n",
      "Confusion matrix [SNII, SNIa]:\n",
      "[[80 30]\n",
      " [25 80]]\n",
      "\n",
      "âœ“ Results saved: ../results/plasticc_classical_results_2k.json\n",
      "\n",
      "======================================================================\n",
      "FEATURE SELECTION (TRAIN ONLY)\n",
      "======================================================================\n",
      "Top 10 correlations (train only):\n",
      "           feature  abs_corr  signed_corr      p_value\n",
      "         time_span  0.279160    -0.279160 8.329884e-17\n",
      "      decline_time  0.265730    -0.265730 2.565207e-15\n",
      "           mag_max  0.160794     0.160794 2.232677e-06\n",
      "           mag_std  0.150796     0.150796 9.275654e-06\n",
      "          mag_mean  0.126317     0.126317 2.094389e-04\n",
      "         mag_range  0.123106     0.123106 3.034252e-04\n",
      "         rise_time  0.104246    -0.104246 2.245916e-03\n",
      "mean_decline_slope  0.068830     0.068830 4.396452e-02\n",
      "         max_slope  0.051471    -0.051471 1.321744e-01\n",
      "         flux_mean  0.037626    -0.037626 2.712214e-01\n",
      "\n",
      "ðŸŽ¯ TOP 3 FEATURES FOR QUANTUM: ['time_span', 'decline_time', 'mag_max']\n",
      "\n",
      "âœ“ Saved: ../results/top_3_features.json\n",
      "âœ“ Saved: ../results/feature_correlations_train_only.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 02_classical_with_16_features.ipynb  (FIXED)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# CatBoost optional (but recommended)\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "    print(\"âš ï¸ CatBoost not available. Install with: pip install catboost\")\n",
    "    print(\"   Error:\", e)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"plasticc\")\n",
    "FEATURES_PATH = os.path.join(DATA_DIR, \"transient_features.csv\")\n",
    "\n",
    "RESULTS_DIR = os.path.join(\"..\", \"results\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "OUT_RESULTS_JSON = os.path.join(RESULTS_DIR, \"plasticc_classical_results_2k.json\")\n",
    "OUT_FEATURE_SELECTION_JSON = os.path.join(RESULTS_DIR, \"top_3_features.json\")\n",
    "OUT_CORR_CSV = os.path.join(RESULTS_DIR, \"feature_correlations_train_only.csv\")\n",
    "\n",
    "feature_cols = [\n",
    "    \"mag_min\", \"mag_max\", \"mag_mean\", \"mag_std\", \"mag_range\",\n",
    "    \"flux_max\", \"flux_mean\", \"flux_std\",\n",
    "    \"time_span\", \"rise_time\", \"decline_time\", \"rise_decline_ratio\",\n",
    "    \"mean_rise_slope\", \"mean_decline_slope\", \"max_slope\",\n",
    "    \"n_points\"\n",
    "]\n",
    "\n",
    "label_map = {\"SNII\": 0, \"SNIa\": 1}\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load\n",
    "# -----------------------------\n",
    "df = pd.read_csv(FEATURES_PATH)\n",
    "\n",
    "missing = [c for c in feature_cols + [\"label\"] if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[\"label\"].map(label_map).values\n",
    "if np.any(pd.isnull(y)):\n",
    "    bad = df.loc[pd.isnull(y), \"label\"].unique()\n",
    "    raise ValueError(f\"Unknown labels found: {bad}\")\n",
    "\n",
    "print(f\"Loaded {len(df)} samples with {len(df.columns)} columns\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df[\"label\"].value_counts())\n",
    "print(f\"\\nFeature matrix: {X.shape}\")\n",
    "print(f\"Labels: {y.shape}, distribution: {np.unique(y, return_counts=True)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Train/test split (ONCE)\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)}, Test: {len(X_test)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Models (pipelines to avoid wrong scaling)\n",
    "# -----------------------------\n",
    "lr_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=3000, C=2.0, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=20,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "if CATBOOST_AVAILABLE:\n",
    "    cb = CatBoostClassifier(\n",
    "        iterations=800,\n",
    "        depth=8,\n",
    "        learning_rate=0.05,\n",
    "        random_seed=RANDOM_STATE,\n",
    "        loss_function=\"Logloss\",\n",
    "        eval_metric=\"AUC\",\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": lr_pipe,\n",
    "    \"Random Forest\": rf,\n",
    "}\n",
    "\n",
    "if CATBOOST_AVAILABLE:\n",
    "    models[\"CatBoost\"] = cb\n",
    "\n",
    "# Ensemble: all estimators receive raw X; pipelines handle preprocessing internally\n",
    "estimators = [(\"lr\", lr_pipe), (\"rf\", rf)]\n",
    "weights = [1, 2]\n",
    "if CATBOOST_AVAILABLE:\n",
    "    estimators.append((\"cb\", cb))\n",
    "    weights.append(3)\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=estimators,\n",
    "    voting=\"soft\",\n",
    "    weights=weights,\n",
    "    n_jobs=-1\n",
    ")\n",
    "models[\"Ensemble\"] = ensemble\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Train + Evaluate\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING + EVALUATION (CLASSICAL)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # probability for AUC\n",
    "    proba = model.predict_proba(X_test)\n",
    "    if proba.ndim == 2 and proba.shape[1] == 2:\n",
    "        scores = proba[:, 1]\n",
    "    else:\n",
    "        scores = np.ravel(proba)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, scores)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    print(f\"Accuracy:          {acc:.3f}\")\n",
    "    print(f\"Balanced accuracy: {bacc:.3f}\")\n",
    "    print(f\"AUC:               {auc:.3f}\")\n",
    "    print(\"Confusion matrix [SNII, SNIa]:\")\n",
    "    print(cm)\n",
    "\n",
    "    results[name] = {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"balanced_accuracy\": float(bacc),\n",
    "        \"auc\": float(auc),\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"tn_fp_fn_tp\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)},\n",
    "    }\n",
    "\n",
    "with open(OUT_RESULTS_JSON, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Results saved: {OUT_RESULTS_JSON}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Feature selection for quantum (TRAIN ONLY, no leakage)\n",
    "# -----------------------------\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "corr_rows = []\n",
    "for i, feat in enumerate(feature_cols):\n",
    "    corr, pval = pointbiserialr(y_train, X_train[:, i])\n",
    "    corr_rows.append({\n",
    "        \"feature\": feat,\n",
    "        \"abs_corr\": float(abs(corr)),\n",
    "        \"signed_corr\": float(corr),\n",
    "        \"p_value\": float(pval),\n",
    "    })\n",
    "\n",
    "corr_df = pd.DataFrame(corr_rows).sort_values(\"abs_corr\", ascending=False)\n",
    "top_3_features = corr_df.head(3)[\"feature\"].tolist()\n",
    "\n",
    "corr_df.to_csv(OUT_CORR_CSV, index=False)\n",
    "with open(OUT_FEATURE_SELECTION_JSON, \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"top_3_features\": top_3_features,\n",
    "            \"method\": \"top3_by_abs_pointbiserial_correlation_train_only\",\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"test_size\": TEST_SIZE,\n",
    "        },\n",
    "        f,\n",
    "        indent=2\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE SELECTION (TRAIN ONLY)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Top 10 correlations (train only):\")\n",
    "print(corr_df.head(10).to_string(index=False))\n",
    "print(\"\\nðŸŽ¯ TOP 3 FEATURES FOR QUANTUM:\", top_3_features)\n",
    "print(f\"\\nâœ“ Saved: {OUT_FEATURE_SELECTION_JSON}\")\n",
    "print(f\"âœ“ Saved: {OUT_CORR_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1449f848-a8d6-4dae-95b8-8111b0f57d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Classical ML â€” PLAsTiCC (Top-3 features, leakage-safe)\n",
      "======================================================================\n",
      "FEATURES_PATH: ../data/plasticc/transient_features.csv\n",
      "RESULTS_DIR: ../results\n",
      "\n",
      "Loaded samples: 1072\n",
      "Label distribution:\n",
      " label\n",
      "SNII    549\n",
      "SNIa    523\n",
      "Name: count, dtype: int64\n",
      "X_full shape: (1072, 16)\n",
      "\n",
      "Train full: (857, 16) Test full: (215, 16)\n",
      "Train label counts: {np.int64(0): np.int64(439), np.int64(1): np.int64(418)}\n",
      "Test  label counts: {np.int64(0): np.int64(110), np.int64(1): np.int64(105)}\n",
      "\n",
      "Top 10 correlations (TRAIN only):\n",
      "           feature  abs_corr  signed_corr      p_value\n",
      "         time_span  0.279160    -0.279160 8.329884e-17\n",
      "      decline_time  0.265730    -0.265730 2.565207e-15\n",
      "           mag_max  0.160794     0.160794 2.232677e-06\n",
      "           mag_std  0.150796     0.150796 9.275654e-06\n",
      "          mag_mean  0.126317     0.126317 2.094389e-04\n",
      "         mag_range  0.123106     0.123106 3.034252e-04\n",
      "         rise_time  0.104246    -0.104246 2.245916e-03\n",
      "mean_decline_slope  0.068830     0.068830 4.396452e-02\n",
      "         max_slope  0.051471    -0.051471 1.321744e-01\n",
      "         flux_mean  0.037626    -0.037626 2.712214e-01\n",
      "\n",
      "ðŸŽ¯ Selected top 3 (TRAIN only): ['time_span', 'decline_time', 'mag_max']\n",
      "âœ“ Saved: ../results/top_3_features_train_only.json\n",
      "âœ“ Saved: ../results/feature_correlations_train_only_top3.csv\n",
      "\n",
      "Clip bounds (TRAIN p1, p99) per feature:\n",
      "  time_span: [17.0575, 393.2386]\n",
      "  decline_time: [3.9602, 377.7529]\n",
      "  mag_max: [-5.8509, -1.0480]\n",
      "Log-transformed features: None\n",
      "\n",
      "======================================================================\n",
      "TRAINING + EVALUATION (CLASSICAL TOP-3, leakage-safe)\n",
      "======================================================================\n",
      "\n",
      "Logistic Regression\n",
      "--------------------------------------------------\n",
      "Accuracy:          0.642\n",
      "Balanced accuracy: 0.643\n",
      "AUC:               0.678\n",
      "Confusion matrix [SNII, SNIa]:\n",
      "[[65 45]\n",
      " [32 73]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        SNII       0.67      0.59      0.63       110\n",
      "        SNIa       0.62      0.70      0.65       105\n",
      "\n",
      "    accuracy                           0.64       215\n",
      "   macro avg       0.64      0.64      0.64       215\n",
      "weighted avg       0.64      0.64      0.64       215\n",
      "\n",
      "\n",
      "Random Forest\n",
      "--------------------------------------------------\n",
      "Accuracy:          0.605\n",
      "Balanced accuracy: 0.605\n",
      "AUC:               0.630\n",
      "Confusion matrix [SNII, SNIa]:\n",
      "[[64 46]\n",
      " [39 66]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        SNII       0.62      0.58      0.60       110\n",
      "        SNIa       0.59      0.63      0.61       105\n",
      "\n",
      "    accuracy                           0.60       215\n",
      "   macro avg       0.61      0.61      0.60       215\n",
      "weighted avg       0.61      0.60      0.60       215\n",
      "\n",
      "\n",
      "CatBoost\n",
      "--------------------------------------------------\n",
      "Accuracy:          0.591\n",
      "Balanced accuracy: 0.591\n",
      "AUC:               0.634\n",
      "Confusion matrix [SNII, SNIa]:\n",
      "[[63 47]\n",
      " [41 64]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        SNII       0.61      0.57      0.59       110\n",
      "        SNIa       0.58      0.61      0.59       105\n",
      "\n",
      "    accuracy                           0.59       215\n",
      "   macro avg       0.59      0.59      0.59       215\n",
      "weighted avg       0.59      0.59      0.59       215\n",
      "\n",
      "\n",
      "Ensemble\n",
      "--------------------------------------------------\n",
      "Accuracy:          0.609\n",
      "Balanced accuracy: 0.610\n",
      "AUC:               0.638\n",
      "Confusion matrix [SNII, SNIa]:\n",
      "[[66 44]\n",
      " [40 65]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        SNII       0.62      0.60      0.61       110\n",
      "        SNIa       0.60      0.62      0.61       105\n",
      "\n",
      "    accuracy                           0.61       215\n",
      "   macro avg       0.61      0.61      0.61       215\n",
      "weighted avg       0.61      0.61      0.61       215\n",
      "\n",
      "\n",
      "âœ“ Saved results to: ../results/plasticc_classical_top3_results.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Classical Top-3 Baseline (LEAKAGE-SAFE, apples-to-apples with quantum)\n",
    "# - Split FIRST (train/test)\n",
    "# - Select top 3 features using TRAIN ONLY (point-biserial correlation)\n",
    "# - Fit preprocessing on TRAIN ONLY (NaN/Inf clean -> clip p1/p99 -> optional log)\n",
    "# - Train models: LR (scaled via Pipeline), RF, CatBoost (optional), Soft Voting Ensemble\n",
    "# - Save: ../results/plasticc_classical_top3_results.json\n",
    "#        ../results/top_3_features_train_only.json\n",
    "#        ../results/feature_correlations_train_only_top3.csv\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# CatBoost optional\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "    print(\"âš ï¸ CatBoost not available. Install with: pip install catboost\")\n",
    "    print(\"   Error:\", e)\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Config\n",
    "# -----------------------------\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"plasticc\")\n",
    "FEATURES_PATH = os.path.join(DATA_DIR, \"transient_features.csv\")\n",
    "\n",
    "RESULTS_DIR = os.path.join(\"..\", \"results\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "OUT_JSON = os.path.join(RESULTS_DIR, \"plasticc_classical_top3_results.json\")\n",
    "OUT_TOP3_JSON = os.path.join(RESULTS_DIR, \"top_3_features_train_only.json\")\n",
    "OUT_CORR_CSV = os.path.join(RESULTS_DIR, \"feature_correlations_train_only_top3.csv\")\n",
    "\n",
    "label_map = {\"SNII\": 0, \"SNIa\": 1}\n",
    "\n",
    "feature_cols = [\n",
    "    \"mag_min\", \"mag_max\", \"mag_mean\", \"mag_std\", \"mag_range\",\n",
    "    \"flux_max\", \"flux_mean\", \"flux_std\",\n",
    "    \"time_span\", \"rise_time\", \"decline_time\", \"rise_decline_ratio\",\n",
    "    \"mean_rise_slope\", \"mean_decline_slope\", \"max_slope\",\n",
    "    \"n_points\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Classical ML â€” PLAsTiCC (Top-3 features, leakage-safe)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURES_PATH:\", FEATURES_PATH)\n",
    "print(\"RESULTS_DIR:\", RESULTS_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load data\n",
    "# -----------------------------\n",
    "df = pd.read_csv(FEATURES_PATH)\n",
    "\n",
    "missing = [c for c in feature_cols + [\"label\"] if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "X_full = df[feature_cols].values\n",
    "y = df[\"label\"].map(label_map).values\n",
    "if np.any(pd.isnull(y)):\n",
    "    bad = df.loc[pd.isnull(y), \"label\"].unique()\n",
    "    raise ValueError(f\"Unknown labels found: {bad}\")\n",
    "\n",
    "print(\"\\nLoaded samples:\", len(df))\n",
    "print(\"Label distribution:\\n\", df[\"label\"].value_counts())\n",
    "print(\"X_full shape:\", X_full.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Split FIRST (no leakage)\n",
    "# -----------------------------\n",
    "X_train_full, X_test_full, y_train, y_test = train_test_split(\n",
    "    X_full, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain full:\", X_train_full.shape, \"Test full:\", X_test_full.shape)\n",
    "print(\"Train label counts:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "print(\"Test  label counts:\", dict(zip(*np.unique(y_test, return_counts=True))))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Select top 3 features using TRAIN ONLY\n",
    "# -----------------------------\n",
    "corr_rows = []\n",
    "for i, feat in enumerate(feature_cols):\n",
    "    corr, pval = pointbiserialr(y_train, X_train_full[:, i])\n",
    "    corr_rows.append({\n",
    "        \"feature\": feat,\n",
    "        \"abs_corr\": float(abs(corr)),\n",
    "        \"signed_corr\": float(corr),\n",
    "        \"p_value\": float(pval),\n",
    "    })\n",
    "\n",
    "corr_df = pd.DataFrame(corr_rows).sort_values(\"abs_corr\", ascending=False)\n",
    "selected_features = corr_df.head(3)[\"feature\"].tolist()\n",
    "selected_idx = [feature_cols.index(f) for f in selected_features]\n",
    "\n",
    "corr_df.to_csv(OUT_CORR_CSV, index=False)\n",
    "with open(OUT_TOP3_JSON, \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"top_3_features\": selected_features,\n",
    "            \"method\": \"top3_by_abs_pointbiserial_correlation_train_only\",\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"test_size\": TEST_SIZE,\n",
    "        },\n",
    "        f,\n",
    "        indent=2\n",
    "    )\n",
    "\n",
    "print(\"\\nTop 10 correlations (TRAIN only):\")\n",
    "print(corr_df.head(10).to_string(index=False))\n",
    "print(\"\\nðŸŽ¯ Selected top 3 (TRAIN only):\", selected_features)\n",
    "print(f\"âœ“ Saved: {OUT_TOP3_JSON}\")\n",
    "print(f\"âœ“ Saved: {OUT_CORR_CSV}\")\n",
    "\n",
    "# Slice train/test to selected features\n",
    "X_train = X_train_full[:, selected_idx].copy()\n",
    "X_test  = X_test_full[:,  selected_idx].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Preprocessing fit on TRAIN only, apply to TEST\n",
    "#    - NaN/Inf clean\n",
    "#    - clip p1/p99 from TRAIN\n",
    "#    - decide log-transform from TRAIN\n",
    "# -----------------------------\n",
    "def clean_clip_log_fit_transform(X_tr, X_te, feature_names):\n",
    "    # Clean NaN/Inf\n",
    "    if np.any(np.isnan(X_tr)) or np.any(np.isinf(X_tr)):\n",
    "        print(\"\\nâš ï¸ Found NaN/Inf in TRAIN â€” cleaning with nan_to_num.\")\n",
    "        X_tr = np.nan_to_num(X_tr, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "\n",
    "    if np.any(np.isnan(X_te)) or np.any(np.isinf(X_te)):\n",
    "        print(\"\\nâš ï¸ Found NaN/Inf in TEST â€” cleaning with nan_to_num.\")\n",
    "        X_te = np.nan_to_num(X_te, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "\n",
    "    X_tr2 = X_tr.copy()\n",
    "    X_te2 = X_te.copy()\n",
    "\n",
    "    clip_bounds = []\n",
    "    log_transformed = []\n",
    "    log_shifts = {}\n",
    "\n",
    "    for j in range(X_tr2.shape[1]):\n",
    "        # Clip bounds from TRAIN only\n",
    "        p1, p99 = np.percentile(X_tr2[:, j], [1, 99])\n",
    "        X_tr2[:, j] = np.clip(X_tr2[:, j], p1, p99)\n",
    "        X_te2[:, j] = np.clip(X_te2[:, j], p1, p99)\n",
    "        clip_bounds.append((float(p1), float(p99)))\n",
    "\n",
    "        # Decide log-transform from TRAIN only (same rule as quantum notebook)\n",
    "        fmin, fmax = X_tr2[:, j].min(), X_tr2[:, j].max()\n",
    "        dynamic_range = (fmax - fmin) / (abs(fmin) + 1e-10)\n",
    "\n",
    "        if dynamic_range > 100:\n",
    "            shift = float(X_tr2[:, j].min())  # TRAIN-only shift\n",
    "            X_tr2[:, j] = np.log1p(X_tr2[:, j] - shift + 1)\n",
    "            X_te2[:, j] = np.log1p(X_te2[:, j] - shift + 1)\n",
    "            log_transformed.append(feature_names[j])\n",
    "            log_shifts[feature_names[j]] = shift\n",
    "\n",
    "    return X_tr2, X_te2, clip_bounds, log_transformed, log_shifts\n",
    "\n",
    "X_train_proc, X_test_proc, clip_bounds, log_transformed, log_shifts = clean_clip_log_fit_transform(\n",
    "    X_train, X_test, selected_features\n",
    ")\n",
    "\n",
    "print(\"\\nClip bounds (TRAIN p1, p99) per feature:\")\n",
    "for feat, (p1, p99) in zip(selected_features, clip_bounds):\n",
    "    print(f\"  {feat}: [{p1:.4f}, {p99:.4f}]\")\n",
    "print(\"Log-transformed features:\", log_transformed if log_transformed else \"None\")\n",
    "if log_transformed:\n",
    "    print(\"Log shifts (TRAIN min after clipping):\", log_shifts)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Models\n",
    "# -----------------------------\n",
    "lr_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=2000, C=2.0, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=20,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": lr_pipe,\n",
    "    \"Random Forest\": rf,\n",
    "}\n",
    "\n",
    "if CATBOOST_AVAILABLE:\n",
    "    cb = CatBoostClassifier(\n",
    "        iterations=800,\n",
    "        depth=8,\n",
    "        learning_rate=0.05,\n",
    "        random_seed=RANDOM_STATE,\n",
    "        loss_function=\"Logloss\",\n",
    "        eval_metric=\"AUC\",\n",
    "        verbose=False\n",
    "    )\n",
    "    models[\"CatBoost\"] = cb\n",
    "\n",
    "# Soft voting ensemble (pipelines handle their own preprocessing)\n",
    "estimators = [(\"lr\", lr_pipe), (\"rf\", rf)]\n",
    "weights = [1, 2]\n",
    "if CATBOOST_AVAILABLE:\n",
    "    estimators.append((\"cb\", cb))\n",
    "    weights.append(3)\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=estimators,\n",
    "    voting=\"soft\",\n",
    "    weights=weights,\n",
    "    n_jobs=-1\n",
    ")\n",
    "models[\"Ensemble\"] = ensemble\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Train + Evaluate\n",
    "# -----------------------------\n",
    "def get_scores(model, X):\n",
    "    proba = model.predict_proba(X)\n",
    "    if proba.ndim == 2 and proba.shape[1] == 2:\n",
    "        return proba[:, 1]\n",
    "    return np.ravel(proba)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING + EVALUATION (CLASSICAL TOP-3, leakage-safe)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = {\n",
    "    \"dataset_size\": int(len(df)),\n",
    "    \"train_size\": int(len(X_train_proc)),\n",
    "    \"test_size\": int(len(X_test_proc)),\n",
    "    \"features_selected_auto\": selected_features,\n",
    "    \"feature_selection_method\": \"top3_by_abs_pointbiserial_correlation_train_only\",\n",
    "    \"preprocessing\": {\n",
    "        \"order\": \"split -> select_features(train) -> fit_preproc(train) -> apply(test)\",\n",
    "        \"outlier_clipping\": \"1st/99th percentile (fit on TRAIN)\",\n",
    "        \"clip_bounds\": {\n",
    "            feat: {\"p1\": float(p1), \"p99\": float(p99)}\n",
    "            for feat, (p1, p99) in zip(selected_features, clip_bounds)\n",
    "        },\n",
    "        \"log_transformed_features\": log_transformed,\n",
    "        \"log_shifts_train\": log_shifts,\n",
    "        \"scaling\": \"StandardScaler inside LR pipeline; RF/CatBoost use raw preprocessed features\",\n",
    "    },\n",
    "    \"models\": {}\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    model.fit(X_train_proc, y_train)\n",
    "    y_pred = model.predict(X_test_proc)\n",
    "    scores = get_scores(model, X_test_proc)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, scores)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    print(f\"Accuracy:          {acc:.3f}\")\n",
    "    print(f\"Balanced accuracy: {bacc:.3f}\")\n",
    "    print(f\"AUC:               {auc:.3f}\")\n",
    "    print(\"Confusion matrix [SNII, SNIa]:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"SNII\", \"SNIa\"], zero_division=0))\n",
    "\n",
    "    results[\"models\"][name] = {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"balanced_accuracy\": float(bacc),\n",
    "        \"auc\": float(auc),\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"tn_fp_fn_tp\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)},\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Save results JSON\n",
    "# -----------------------------\n",
    "with open(OUT_JSON, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Saved results to: {OUT_JSON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0782e-155e-433a-bba5-9f3567df10a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
