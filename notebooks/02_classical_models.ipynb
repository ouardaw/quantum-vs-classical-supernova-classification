{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2c17e3-1d4a-4296-b715-3f1f6bed7759",
   "metadata": {},
   "source": [
    "## What this notebook does (Classical Baselines)\n",
    "\n",
    "This notebook builds **classical machine learning baselines** for the PLAsTiCC supernova subset (SNIa vs SNII) using the **full engineered feature set**. The goal is to establish a strong reference point before testing a quantum classifier.\n",
    "\n",
    "### Steps in this notebook\n",
    "\n",
    "1. **Load engineered features**\n",
    "   - Reads `../data/plasticc/transient_features.csv`, which contains one row per transient and the precomputed light-curve features.\n",
    "\n",
    "2. **Prepare the dataset**\n",
    "   - Uses the full set of **16 engineered features** (magnitude, flux, timing, slopes, and `n_points`).\n",
    "   - Converts labels from strings to numeric targets: **SNII â†’ 0**, **SNIa â†’ 1**.\n",
    "\n",
    "3. **Train/test split**\n",
    "   - Splits the data into **80% train / 20% test** with stratification to preserve the class balance.\n",
    "\n",
    "4. **Scaling**\n",
    "   - Applies `StandardScaler` for models that benefit from standardized inputs (Logistic Regression and the Ensemble).\n",
    "\n",
    "5. **Train classical models**\n",
    "   - **Logistic Regression**: linear baseline on scaled features.\n",
    "   - **Random Forest**: non-linear baseline using raw (unscaled) features.\n",
    "   - **CatBoost**: gradient boosting baseline using raw (unscaled) features.\n",
    "   - **Soft Voting Ensemble**: combines LR + RF + CatBoost using probability averaging.\n",
    "\n",
    "6. **Evaluate performance**\n",
    "   - Reports **Accuracy** and **ROC AUC** for each model.\n",
    "   - Prints a **confusion matrix** to show SNII vs SNIa errors.\n",
    "\n",
    "7. **Save results for reproducibility**\n",
    "   - Writes metrics to `../results/plasticc_classical_results_2k.json` so they can be referenced later in the README and compared directly to the quantum notebook.\n",
    "\n",
    "### Output / takeaway\n",
    "\n",
    "At the end, youâ€™ll have a clear classical performance baseline (including an ensemble) that serves as the benchmark for the **quantum ML comparison** in Notebook 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa10230-1b2d-474e-b523-4ceef7a0f4f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1072 samples with 18 columns\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "SNII    549\n",
      "SNIa    523\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Feature matrix: (1072, 16)\n",
      "Labels: (1072,), distribution: (array([0, 1]), array([549, 523]))\n",
      "\n",
      "Train: 857, Test: 215\n",
      "\n",
      "======================================================================\n",
      "TRAINING CLASSICAL MODELS\n",
      "======================================================================\n",
      "\n",
      "1. Logistic Regression...\n",
      "2. Random Forest...\n",
      "3. CatBoost...\n",
      "4. Ensemble (Voting)...\n",
      "\n",
      "======================================================================\n",
      "CLASSICAL MODEL RESULTS\n",
      "======================================================================\n",
      "\n",
      "Logistic Regression:\n",
      "--------------------------------------------------\n",
      "Accuracy: 71.2% (0.712)\n",
      "AUC:      77.0% (0.770)\n",
      "\n",
      "Confusion Matrix:\n",
      "              SNII  SNIa\n",
      "Actual SNII    75    35\n",
      "       SNIa    27    78\n",
      "\n",
      "Random Forest:\n",
      "--------------------------------------------------\n",
      "Accuracy: 75.8% (0.758)\n",
      "AUC:      84.5% (0.845)\n",
      "\n",
      "Confusion Matrix:\n",
      "              SNII  SNIa\n",
      "Actual SNII    81    29\n",
      "       SNIa    23    82\n",
      "\n",
      "CatBoost:\n",
      "--------------------------------------------------\n",
      "Accuracy: 74.4% (0.744)\n",
      "AUC:      84.6% (0.846)\n",
      "\n",
      "Confusion Matrix:\n",
      "              SNII  SNIa\n",
      "Actual SNII    83    27\n",
      "       SNIa    28    77\n",
      "\n",
      "Ensemble:\n",
      "--------------------------------------------------\n",
      "Accuracy: 74.4% (0.744)\n",
      "AUC:      85.2% (0.852)\n",
      "\n",
      "Confusion Matrix:\n",
      "              SNII  SNIa\n",
      "Actual SNII    80    30\n",
      "       SNIa    25    80\n",
      "\n",
      "âœ“ Results saved: ../results/plasticc_classical_results_2k.json\n",
      "\n",
      "======================================================================\n",
      "CLASSICAL TRAINING COMPLETE!\n",
      "======================================================================\n",
      "Next step: Train quantum model with top 3 features\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"plasticc\")\n",
    "features_df = pd.read_csv(os.path.join(DATA_DIR, \"transient_features.csv\"))\n",
    "\n",
    "print(f\"Loaded {len(features_df)} samples with {len(features_df.columns)} columns\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(features_df['label'].value_counts())\n",
    "\n",
    "# ============================================================================\n",
    "# PREPARE DATA - USE ALL 16 FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "feature_cols = [\n",
    "    # Magnitude features\n",
    "    \"mag_min\", \"mag_max\", \"mag_mean\", \"mag_std\", \"mag_range\",\n",
    "    # Flux features\n",
    "    \"flux_max\", \"flux_mean\", \"flux_std\",\n",
    "    # Time features\n",
    "    \"time_span\", \"rise_time\", \"decline_time\", \"rise_decline_ratio\",\n",
    "    # Slope features\n",
    "    \"mean_rise_slope\", \"mean_decline_slope\", \"max_slope\",\n",
    "    # Metadata\n",
    "    \"n_points\"\n",
    "]\n",
    "\n",
    "X = features_df[feature_cols].values\n",
    "y_str = features_df['label'].values\n",
    "\n",
    "label_map = {'SNII': 0, 'SNIa': 1}\n",
    "y = np.array([label_map[label] for label in y_str])\n",
    "\n",
    "print(f\"\\nFeature matrix: {X.shape}\")\n",
    "print(f\"Labels: {y.shape}, distribution: {np.unique(y, return_counts=True)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN-TEST SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {X_train.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SCALE FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING CLASSICAL MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "models = {}\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"\\n1. Logistic Regression...\")\n",
    "lr = LogisticRegression(max_iter=2000, C=2.0, random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "models['Logistic Regression'] = lr\n",
    "\n",
    "# Random Forest\n",
    "print(\"2. Random Forest...\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "models['Random Forest'] = rf\n",
    "\n",
    "# CatBoost\n",
    "print(\"3. CatBoost...\")\n",
    "cb = CatBoostClassifier(\n",
    "    iterations=800,\n",
    "    depth=8,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "cb.fit(X_train, y_train)\n",
    "models['CatBoost'] = cb\n",
    "\n",
    "# Ensemble\n",
    "print(\"4. Ensemble (Voting)...\")\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', lr),\n",
    "        ('rf', rf),\n",
    "        ('cb', cb)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[1, 2, 3]\n",
    ")\n",
    "ensemble.fit(X_train_scaled, y_train)\n",
    "models['Ensemble'] = ensemble\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATE ALL MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLASSICAL MODEL RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Use scaled features for LR and Ensemble, raw for RF and CB\n",
    "    if name in ['Logistic Regression', 'Ensemble']:\n",
    "        X_test_input = X_test_scaled\n",
    "    else:\n",
    "        X_test_input = X_test\n",
    "    \n",
    "    y_pred = model.predict(X_test_input)\n",
    "    y_proba = model.predict_proba(X_test_input)[:, 1]\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.1%} ({acc:.3f})\")\n",
    "    print(f\"AUC:      {auc:.1%} ({auc:.3f})\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"              SNII  SNIa\")\n",
    "    print(f\"Actual SNII   {cm[0,0]:3d}   {cm[0,1]:3d}\")\n",
    "    print(f\"       SNIa   {cm[1,0]:3d}   {cm[1,1]:3d}\")\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': float(acc),\n",
    "        'auc': float(auc),\n",
    "        'confusion_matrix': cm.tolist()\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "RESULTS_DIR = os.path.join(\"..\", \"results\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "results_path = os.path.join(RESULTS_DIR, \"plasticc_classical_results_2k.json\")\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Results saved: {results_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLASSICAL TRAINING COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Next step: Train quantum model with top 3 features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cced9-bb19-45d8-a135-d6eb8c0f47d0",
   "metadata": {},
   "source": [
    "## Feature Correlation Analysis (Preparation for Quantum ML)\n",
    "\n",
    "Before moving to the quantum classifier, we analyze which engineered features\n",
    "are most strongly correlated with the target label (SNIa vs SNII).\n",
    "\n",
    "This step serves two purposes:\n",
    "1. **Dimensionality reduction** â€“ quantum models are limited to a small number\n",
    "   of input features due to qubit constraints.\n",
    "2. **Fair comparison** â€“ ensures the quantum model uses the *best available*\n",
    "   features rather than an arbitrary subset.\n",
    "\n",
    "We use point-biserial correlation (appropriate for binary labels) to rank\n",
    "all features and identify the top 3 candidates for quantum encoding.\n",
    "\n",
    "The selected features and their class separation statistics are saved and\n",
    "reused directly in `03_quantum_classifier.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e1d35-b885-4b3c-a4d4-72c6b90940ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE CORRELATION ANALYSIS - FIND TOP 3 FOR QUANTUM\n",
    "# ============================================================================\n",
    "\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE CORRELATION WITH LABEL (SNIa vs SNII)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Numeric encoding: SNII=0, SNIa=1\n",
    "label_numeric = y\n",
    "\n",
    "correlations = []\n",
    "for i, feat in enumerate(feature_cols):\n",
    "    corr, pval = pointbiserialr(label_numeric, X[:, i])\n",
    "    correlations.append({\n",
    "        'feature': feat,\n",
    "        'correlation': abs(corr),  # absolute value for ranking\n",
    "        'correlation_signed': corr,\n",
    "        'p_value': pval\n",
    "    })\n",
    "\n",
    "corr_df = pd.DataFrame(correlations).sort_values('correlation', ascending=False)\n",
    "\n",
    "print(\"\\nFeatures ranked by correlation strength:\")\n",
    "print(corr_df.to_string(index=False))\n",
    "\n",
    "# Select TOP 3\n",
    "top_3_features = corr_df.head(3)['feature'].tolist()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"ðŸŽ¯ TOP 3 FEATURES FOR QUANTUM: {top_3_features}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show class separation for these features\n",
    "print(\"\\nClass separation check:\")\n",
    "for feat in top_3_features:\n",
    "    feat_idx = feature_cols.index(feat)\n",
    "    feat_data = X[:, feat_idx]\n",
    "    \n",
    "    snia_vals = feat_data[y == 1]\n",
    "    snii_vals = feat_data[y == 0]\n",
    "    \n",
    "    print(f\"\\n{feat}:\")\n",
    "    print(f\"  SNIa: mean={snia_vals.mean():.3f}, std={snia_vals.std():.3f}\")\n",
    "    print(f\"  SNII: mean={snii_vals.mean():.3f}, std={snii_vals.std():.3f}\")\n",
    "    print(f\"  Separation: {abs(snia_vals.mean() - snii_vals.mean()):.3f} ({abs(snia_vals.mean() - snii_vals.mean()) / snii_vals.std():.2f} Ïƒ)\")\n",
    "\n",
    "# Save for quantum notebook\n",
    "top_3_path = os.path.join(RESULTS_DIR, \"top_3_features.json\")\n",
    "with open(top_3_path, 'w') as f:\n",
    "    json.dump({'top_3_features': top_3_features}, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Saved top 3 features to: {top_3_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
